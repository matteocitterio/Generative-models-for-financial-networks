{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f732f01b-2e53-4c13-93f2-2c269086dc3d",
   "metadata": {},
   "source": [
    "# GraphVAE for networks generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401a54a-1741-490a-820d-50f5848229e4",
   "metadata": {},
   "source": [
    "### Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b8d42-3217-4b51-bc3a-d8d5a5678b1c",
   "metadata": {},
   "source": [
    "First of all let's have a brief remind on **VAE** Variational autoencoders, firstly introduced by [*D. P. Kingma and M. Welling. 'Auto-encoding variational bayes', 2014*](https://arxiv.org/pdf/1312.6114.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394e167-ab79-469a-9ce9-4f50ef7fbc13",
   "metadata": {},
   "source": [
    "VAE is a neural network architecture belonging to the family of variational Bayesian methods.\n",
    "\n",
    "From a probabilistic point of view we want to maximize the likelyhood of our data **x** given a proper set of parameters **$\\theta$**, like in a normal MLE problem: $p_{\\theta}(x) = p(x|\\theta)$. By neglecting from the third moment upwards, we could approximate the distribution to a normal distribution $\\mathcal{N}(x|\\mu,\\sigma)$. Simple distributions like the normal ones are usually easy to maximize, however if we assume a prior over a latent space $z$ the posterior usually becomes intractable.\n",
    "\n",
    "By marginalizing over $z$ we obtain:\n",
    "\n",
    "$$p_{\\theta}(x) = \\int_{\\mathcal{Z}}{p_{\\theta}(x,z)dz} = \\int_{\\mathcal{Z}}{p_{\\theta}(x|z)p_{\\theta}(z)dz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625612e2-6f63-430a-a354-6bc3b423bb38",
   "metadata": {},
   "source": [
    "So we may define the set of relationships between the input data and the latent space through:\n",
    "- $p_{\\theta}(z)$ the prior distribution of the latent space\n",
    "- $p_{\\theta}(x|z)$ the likelyhood\n",
    "- $p_{\\theta}(z|x)$ the posterior\n",
    "\n",
    "Using the Bayes's theorem we could get:\n",
    "\n",
    "$$p_{\\theta}(z|x) = \\frac{p_{\\theta}(x|z)p_{\\theta}(z)}{p_{\\theta}(x)}$$\n",
    "\n",
    "but the the computation is usually expensive if not intractable. However, it is possible to approximate the posterior:\n",
    "\n",
    "$$ q_{\\phi}(z|x)\\simeq p_{\\theta}(z|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7df44d-d079-463a-9686-e4958fef9b3c",
   "metadata": {},
   "source": [
    "### Variational Graph Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c97c73-06bb-4663-b0fe-e90fff39dbe3",
   "metadata": {},
   "source": [
    "Variational Graph Autoencoders [Kingma and Welling, 2016](https://arxiv.org/pdf/1611.07308.pdf) provide a framework extension to graph for VAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157764c3-ced9-4330-91f5-2b345a6705c4",
   "metadata": {},
   "source": [
    "Our problem could be formalized as follows: an undirected graph $\\mathcal{G}=(\\nu, \\epsilon)$ with $N$ nodes and a features/attribute matrix $X\\in\\mathbb{R}^{N\\times C}$. An adjacency matrix $A\\in\\mathbb{R}^{N\\times N}$ with self-loops included. Assume that each node within the graph is associated to a latent variable $\\in Z$ with $Z\\in\\mathbb{R}^{N\\times F}$ and $F$ being the latent space dimension, we are interested in inferring the latent variables of nodes in the graph and decoding the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a1c1a-9eb6-4b39-82aa-4edbc6696fc6",
   "metadata": {},
   "source": [
    "Similarly to VAE, VGAE consist of an **encoder** $q_{\\phi}(Z|A,X)$, a **decoder** $p_{\\theta}(A|Z)$ and a prior $p(Z)$.\n",
    "- The **encoder** tries to learn a distribution of latent variables associated with each node conditioning on the node features $X$ and $A$. One efficient option is to instantiate $q_{\\phi}(Z|A,X)$ as a graph neural network where the learnable parameters are $\\phi$. In particular, VGAE assumes a node-independent encoder so that the probabilities factorize: $$q_{\\phi}(Z|A,X) = \\prod_{i=1}^{N}q_{\\phi}(z_{i}|A,X)$$ then, by neglecting from the third moment upwards of your distribution, the problem translates into: $$q_{\\phi}(z_{i}|A,X)=\\mathcal{N}(z_{i}|\\mu_{i},diag(\\sigma_{i}^2))$$ $$\\mathbf{\\mu},\\mathbf{\\sigma} = GCN_{\\phi}(X,A)$$ Where $z_{i}, \\mu_{i},\\sigma_{i}$ are the i-th rows of the matrices $Z,\\mu$ and $\\sigma$. The mean and diagonal covariance are predicted by the encoder network, i.e. the $GCN$. For a two-layer $GCN$ we have: $$ H=\\tilde{A}\\sigma{(\\tilde{A}XW_{1})}W_{2}$$ where $H\\in\\mathbb{R}^{N\\times d_{H}}$ are the node representations (each node is associated with a size $d_{H}$ vector), $\\tilde{A}=D^{-\\frac{1}{2}}(A+I)D^{-\\frac{1}{2}}$ is the normalized adjacency matrix as described by the [original 2016 GCN paper by Kipf and Welling](https://arxiv.org/abs/1609.02907). $\\sigma$ is a pointwise nonlinearity (e.g. a ReLU) and $\\{W_{1},W_{2}\\}$ are trainable parameters containing the biases. Relying on the learned node representation, the distribution is computed as follows: $$q_{\\phi}(Z|A,X) = \\prod_{i=1}^{N}q_{\\phi}(z_{i}|A,X)$$ $$q_{\\phi}(z_{i}|A,X)=\\mathcal{N}(z_{i}|\\mu_{i},\\sigma_{i}^2I)$$ $$\\mu=MPL_{\\mu}(H)$$ $$\\log{\\sigma}=MPL_{\\sigma}{(H)}$$ Where $\\mu_{i},\\sigma_{i}$ are the i-th rows of the MPL predictions. Therefore, the set $\\phi$ of parameters consist in the set of the trainable parameters of the twp MLPs and the aforementioned GCN. We remark that the NNs underlying each Gaussian ('GNN+MLP') are very powerful so that the conditional distributions are expressive in capturing the uncertainty of latent variables and computationally cheaper than other techniques.\n",
    "- GVAEs often adopt a **prior** that remains fixed during the training. A common choice is a node-independent Gaussian as follows: $$p(Z)=\\prod_{i}^{N}{p(z_{i})}$$ $$p(z_i)=\\mathcal{N}(0,I)$$ Surely this prior can be substituted by more powerful models such as autoregressive models at the cost of more computational resources. Nevertheless, a simple prior like the one expressed before is usually the starting point to benchmark more complicated alternatives.\n",
    "- The aim of a **decoder** is to construct a probability distribution over the graph and it's features/attributes conditioned on the latent variables, $p(\\mathcal{G}|Z)$. One should always consider all the possible node permutations, each corresponding to an adjacency matrix with different rows orderings which leaves the graph unchanged: $$ p(\\mathcal{G}|Z) = \\sum_{P\\in\\prod_{\\mathcal{G}}} {p(PAP^{T},PX|Z)}$$ but we'll neglect this discussion for the moment. A simple and popular construction of the probability distribution could be: $$ p(A,X|Z)=\\prod_{i,j}p(A_{ij}|Z)\\prod_{i=1}^{N}p(x_i|Z)$$ $$p(A_{ij}|Z)=Bernoulli(\\Theta_{ij})$$ $$p(x_i|Z)=\\mathcal{N}(\\tilde{\\mu}_{i},\\tilde{\\sigma}_i)$$ Where, once again, the parameters are learned through MLPs: $$\\Theta_{ij}=MLP_{\\Theta}([z_{i}||z_j])$$ $$\\tilde{\\mu}_{i}=MLP_{\\tilde{\\mu}}(z_i)$$ $$\\tilde{\\sigma}_{i}=MLP_{\\tilde{\\sigma}}(z_i)$$\n",
    "- The **objective** of the GVAE is the evidence lower bound (ELBO): $$\\max_{\\theta,\\phi}{\\mathbb{E}_{q_{\\phi}(Z|A,X)} {[\\log{p_{\\theta}(\\mathcal{G}|Z)}} - KL(q_{\\phi}(Z|A,X)||p(Z))]}$$ where the Kullback-Leibler divergence measures the divergence between two probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e06a4d-4b5a-4c68-a219-79741e0c8ec0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
