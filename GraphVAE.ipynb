{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f732f01b-2e53-4c13-93f2-2c269086dc3d",
   "metadata": {},
   "source": [
    "# GraphVAE for networks generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401a54a-1741-490a-820d-50f5848229e4",
   "metadata": {},
   "source": [
    "### Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b8d42-3217-4b51-bc3a-d8d5a5678b1c",
   "metadata": {},
   "source": [
    "First of all let's have a brief remind on **VAE** Variational autoencoders, firstly introduced by [*D. P. Kingma and M. Welling. 'Auto-encoding variational bayes', 2014*](https://arxiv.org/pdf/1312.6114.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394e167-ab79-469a-9ce9-4f50ef7fbc13",
   "metadata": {},
   "source": [
    "VAE is a neural network architecture belonging to the family of variational Bayesian methods.\n",
    "\n",
    "From a probabilistic point of view we want to maximize the likelyhood of our data **x** given a proper set of parameters **$\\theta$**, like in a normal MLE problem: $p_{\\theta}(x) = p(x|\\theta)$. By neglecting from the third moment upwards, we could approximate the distribution to a normal distribution $\\mathcal{N}(x|\\mu,\\sigma)$. Simple distributions like the normal ones are usually easy to maximize, however if we assume a prior over a latent space $z$ the posterior usually becomes intractable.\n",
    "\n",
    "By marginalizing over $z$ we obtain:\n",
    "\n",
    "$$p_{\\theta}(x) = \\int_{\\mathcal{Z}}{p_{\\theta}(x,z)dz} = \\int_{\\mathcal{Z}}{p_{\\theta}(x|z)p_{\\theta}(z)dz}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625612e2-6f63-430a-a354-6bc3b423bb38",
   "metadata": {},
   "source": [
    "So we may define the set of relationships between the input data and the latent space through:\n",
    "- $p_{\\theta}(z)$ the prior distribution of the latent space\n",
    "- $p_{\\theta}(x|z)$ the likelyhood\n",
    "- $p_{\\theta}(z|x)$ the posterior\n",
    "\n",
    "Using the Bayes's theorem we could get:\n",
    "\n",
    "$$p_{\\theta}(z|x) = \\frac{p_{\\theta}(x|z)p_{\\theta}(z)}{p_{\\theta}(x)}$$\n",
    "\n",
    "but the the computation is usually expensive if not intractable. However, it is possible to approximate the posterior:\n",
    "\n",
    "$$ q_{\\phi}(z|x)\\simeq p_{\\theta}(z|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7df44d-d079-463a-9686-e4958fef9b3c",
   "metadata": {},
   "source": [
    "### Variational Graph Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c97c73-06bb-4663-b0fe-e90fff39dbe3",
   "metadata": {},
   "source": [
    "Variational Graph Autoencoders [Kingma and Welling, 2016](https://arxiv.org/pdf/1611.07308.pdf) provide a framework extension to graph for VAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157764c3-ced9-4330-91f5-2b345a6705c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d40441-ef81-43a7-8ca5-00f4b3227af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
